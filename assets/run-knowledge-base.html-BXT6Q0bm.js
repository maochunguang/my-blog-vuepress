import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as c,o as l,c as o,b as p,d as a,e as n,f as s,a as t}from"./app-CXCJQfm0.js";const r={},d=a("p",null,"🤖️ 一种利用 langchain 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案。",-1),h=t(`<h2 id="langchain-chatchat搭建知识库原理" tabindex="-1"><a class="header-anchor" href="#langchain-chatchat搭建知识库原理"><span>langchain-chatchat搭建知识库原理</span></a></h2><p>🤖️ 一种利用 langchain 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案。</p><p>实现原理如下图所示，过程包括加载文件 -&gt; 读取文本 -&gt; 文本分割 -&gt; 文本向量化 -&gt; 问句向量化 -&gt; 在文本向量中匹配出与问句向量最相似的 top k个 -&gt; 匹配出的文本作为上下文和问题一起添加到 prompt中 -&gt; 提交给 LLM生成回答。</p><figure><img src="https://blog-pics-1252092369.cos.ap-beijing.myqcloud.com/langchain-chatglm.png" alt="原理图" tabindex="0" loading="lazy"><figcaption>原理图</figcaption></figure><h2 id="环境准备" tabindex="-1"><a class="header-anchor" href="#环境准备"><span>环境准备</span></a></h2><h3 id="硬件准备" tabindex="-1"><a class="header-anchor" href="#硬件准备"><span>硬件准备：</span></a></h3><p>win11+wsl2，显卡最好是N卡，我的是2080ti22g（魔改版），这款性价比很高，推荐购买。</p><h3 id="软件准备" tabindex="-1"><a class="header-anchor" href="#软件准备"><span>软件准备：</span></a></h3><ol><li>安装python环境管理工具，<code>pyenv</code>或者<code>conda</code>，我这里用的<code>pyenv</code>。</li><li>安装好python 3.10版本，</li><li>安装好显卡驱动和cuda，根据显卡来安装版本，在win11下安装就行。</li><li>安装好git</li><li>有梯子的话建议整一个。</li></ol><h4 id="验证显卡-python和cuda" tabindex="-1"><a class="header-anchor" href="#验证显卡-python和cuda"><span>验证显卡，python和cuda</span></a></h4><p>方法一、<br> 打开命令提示符或 PowerShell。输入以下命令：<code>nvidia-smi</code>在输出中，可以看到显卡的名称和支持的 CUDA 版本。</p><p>方法二、<br> 使用 Python导入 torch 库。<br> 使用<code>torch.cuda.is_available()</code> 函数来检查是否支持 CUDA。<br> 使用<code>torch.cuda.get_device_name(0)</code> 函数来获取第一个显卡的名称。<br> 使用<code>torch.cuda.get_device_properties(0)</code>函数来获取第一个显卡的属性，其中包括支持的 CUDA 版本。<br> 示例代码：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch

<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;支持 CUDA，第一个显卡名称：</span><span class="token interpolation"><span class="token punctuation">{</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>get_device_name<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;第一个显卡支持的 CUDA 版本：</span><span class="token interpolation"><span class="token punctuation">{</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>get_device_properties<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda_version<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;不支持 CUDA&quot;</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这里可能有个小坑，如果win11里可以看到gpu可用，但是wsl2中无可用cpu，可以把wsl2先停掉，再重新启动。</p><h2 id="本地搭建步骤" tabindex="-1"><a class="header-anchor" href="#本地搭建步骤"><span>本地搭建步骤</span></a></h2><h3 id="_1、复制项目" tabindex="-1"><a class="header-anchor" href="#_1、复制项目"><span>1、复制项目</span></a></h3><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">git</span> clone https://github.com/chatchat-space/Langchain-Chatchat.git<span class="token punctuation">;</span> 
<span class="token builtin class-name">cd</span> Langchain-Chatchat
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2、创建环境" tabindex="-1"><a class="header-anchor" href="#_2、创建环境"><span>2、创建环境</span></a></h3><p>这里不管你用什么虚拟环境管理都可以，一定用一个单独的环境，防止依赖冲突和报错。这里是以<code>pyenv</code>为例子。</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>python <span class="token parameter variable">-m</span> venv test_langchain_chat
<span class="token builtin class-name">source</span> test_langchain_chat/bin/activate
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_3、安装依赖" tabindex="-1"><a class="header-anchor" href="#_3、安装依赖"><span>3、安装依赖</span></a></h3><p>这里安装的适合如果下载的太慢，可以使用安装源，</p><h4 id="a、-使用-i参数安装" tabindex="-1"><a class="header-anchor" href="#a、-使用-i参数安装"><span>a、 使用-i参数安装</span></a></h4><p><code>pip install -i https://pypi.tuna.tsinghua.edu.cn/simple/ package_name</code>。</p><h4 id="b、全局设置安装源" tabindex="-1"><a class="header-anchor" href="#b、全局设置安装源"><span>b、全局设置安装源</span></a></h4><p>可以修改 pip 的配置文件 pip.conf 来全局指定 pip 镜像源。pip.conf 文件位于用户目录下的 .config/pip 目录中。<br> 在 pip.conf 文件中，添加以下内容：</p><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>[global]
index-url = https://pypi.tuna.tsinghua.edu.cn/simple/
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>保存并关闭 pip.conf 文件后，重启 pip。</p><h4 id="c、安装所有依赖" tabindex="-1"><a class="header-anchor" href="#c、安装所有依赖"><span>c、安装所有依赖</span></a></h4><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token comment"># 安装全部依赖</span>
pip <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements.txt 
pip <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements_api.txt
pip <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements_webui.txt 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_4、下载模型" tabindex="-1"><a class="header-anchor" href="#_4、下载模型"><span>4、下载模型</span></a></h3><p>在本地或离线环境下运行，需要首先将项目所需的模型下载至本地，通常开源 LLM 与 Embedding 模型可以从 HuggingFace下载。以本项目中默认使用的 LLM 模型 THUDM/ChatGLM3-6B 与 Embedding 模型 BAAI/bge-large-zh 为例：</p><p>下载模型常用的网站有以下几个，</p>`,33),u={href:"https://huggingface.co/",target:"_blank",rel:"noopener noreferrer"},g={href:"https://www.modelscope.cn/models",target:"_blank",rel:"noopener noreferrer"},m=a("p",null,"下载方式有以下几种",-1),b=a("h4",{id:"a、git-lfs下载",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#a、git-lfs下载"},[a("span",null,"a、git lfs下载")])],-1),v=a("br",null,null,-1),f={href:"https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage",target:"_blank",rel:"noopener noreferrer"},_=t(`<div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">git</span> lfs <span class="token function">install</span>
<span class="token function">git</span> clone https://huggingface.co/THUDM/chatglm3-6b
<span class="token function">git</span> clone https://huggingface.co/BAAI/bge-large-zh
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="b、使用huggingface-hub下载" tabindex="-1"><a class="header-anchor" href="#b、使用huggingface-hub下载"><span>b、使用huggingface_hub下载</span></a></h4>`,2),k={href:"https://hf-mirror.com/docs/huggingface_hub/guides/download",target:"_blank",rel:"noopener noreferrer"},y=t(`<div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>pip <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> huggingface_hub
from huggingface_hub <span class="token function">import</span> hf_hub_download
hf_hub_download<span class="token punctuation">(</span>repo_id<span class="token operator">=</span><span class="token string">&quot;lysandre/arxiv-nlp&quot;</span>, <span class="token assign-left variable">filename</span><span class="token operator">=</span><span class="token string">&quot;config.json&quot;</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="c、使用modelscope-cli下载" tabindex="-1"><a class="header-anchor" href="#c、使用modelscope-cli下载"><span>c、使用modelscope-cli下载</span></a></h4><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token comment">## 安装下载客户端</span>
pip <span class="token function">install</span> modelscope-cli

<span class="token comment">## 下载模型</span>
modelscope download bert-base-chinese
 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_5、修改和初始化配置" tabindex="-1"><a class="header-anchor" href="#_5、修改和初始化配置"><span>5、修改和初始化配置</span></a></h3><h4 id="a、初始化配置" tabindex="-1"><a class="header-anchor" href="#a、初始化配置"><span>a、初始化配置</span></a></h4><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token comment">## 复制配置文件</span>
python copy_config_example.py
<span class="token comment">## 初始化知识库</span>
python init_database.py --recreate-vs
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="b、修改模型配置" tabindex="-1"><a class="header-anchor" href="#b、修改模型配置"><span>b、修改模型配置</span></a></h4><p>修改<code>configs/model_config.py</code><br> 建议把所有的模型放到一个文件夹，后续如果玩其他大模型指定以下目录就行了。</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>MODEL_ROOT_PATH <span class="token operator">=</span> <span class="token string">&quot;/home/xx/soft/ai-models&quot;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="_6、启动调试" tabindex="-1"><a class="header-anchor" href="#_6、启动调试"><span>6、启动调试</span></a></h3>`,10),x={href:"http://localhost:8501/",target:"_blank",rel:"noopener noreferrer"},w=t(`<div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>python startup.py <span class="token parameter variable">-a</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="_7、知识库测试" tabindex="-1"><a class="header-anchor" href="#_7、知识库测试"><span>7、知识库测试</span></a></h3><p>我这里上传了基本epub书籍，都是金融相关的。搜索什么是指数基金是可以显示知识库来源的</p><figure><img src="https://blog-pics-1252092369.cos.ap-beijing.myqcloud.com/v2-cfc6b68f151b1ea8148093e0034ad686_1440w.webp" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><figure><img src="https://blog-pics-1252092369.cos.ap-beijing.myqcloud.com/v2-d3494e31e9a8c04fcd98da270adef82d_1440w.webp" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure>`,5);function q(C,A){const e=c("ExternalLinkIcon");return l(),o("div",null,[d,p(" more "),h,a("ol",null,[a("li",null,[a("a",u,[n("https://huggingface.co/"),s(e)])]),a("li",null,[a("a",g,[n("https://www.modelscope.cn/models"),s(e)])])]),m,b,a("p",null,[n("先安装git lfs，如下："),v,a("a",f,[n("https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage"),s(e)])]),_,a("p",null,[n("详细的教程可以参考："),a("a",k,[n("https://hf-mirror.com/docs/huggingface_hub/guides/download"),s(e)])]),y,a("p",null,[n("执行命令启动服务；访问"),a("a",x,[n("http://localhost:8501/"),s(e)])]),w])}const D=i(r,[["render",q],["__file","run-knowledge-base.html.vue"]]),z=JSON.parse('{"path":"/aigc/run-knowledge-base.html","title":"本地搭建chatgpt知识库","lang":"zh-CN","frontmatter":{"title":"本地搭建chatgpt知识库","icon":"fab fa-markdown","order":2,"category":["AIGC"],"tag":["langchain","chatgpt"],"description":"🤖️ 一种利用 langchain 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案。 langchain-chatchat搭建知识库原理 🤖️ 一种利用 langchain 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方...","head":[["meta",{"property":"og:url","content":"https://codingmore.site/aigc/run-knowledge-base.html"}],["meta",{"property":"og:site_name","content":"Coding More"}],["meta",{"property":"og:title","content":"本地搭建chatgpt知识库"}],["meta",{"property":"og:description","content":"🤖️ 一种利用 langchain 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案。 langchain-chatchat搭建知识库原理 🤖️ 一种利用 langchain 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://blog-pics-1252092369.cos.ap-beijing.myqcloud.com/langchain-chatglm.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-09-23T14:49:47.000Z"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:alt","content":"本地搭建chatgpt知识库"}],["meta",{"property":"article:author","content":"Tommy"}],["meta",{"property":"article:tag","content":"langchain"}],["meta",{"property":"article:tag","content":"chatgpt"}],["meta",{"property":"article:modified_time","content":"2024-09-23T14:49:47.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"本地搭建chatgpt知识库\\",\\"image\\":[\\"https://blog-pics-1252092369.cos.ap-beijing.myqcloud.com/langchain-chatglm.png\\",\\"https://blog-pics-1252092369.cos.ap-beijing.myqcloud.com/v2-cfc6b68f151b1ea8148093e0034ad686_1440w.webp\\",\\"https://blog-pics-1252092369.cos.ap-beijing.myqcloud.com/v2-d3494e31e9a8c04fcd98da270adef82d_1440w.webp\\"],\\"dateModified\\":\\"2024-09-23T14:49:47.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Tommy\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[{"level":2,"title":"langchain-chatchat搭建知识库原理","slug":"langchain-chatchat搭建知识库原理","link":"#langchain-chatchat搭建知识库原理","children":[]},{"level":2,"title":"环境准备","slug":"环境准备","link":"#环境准备","children":[{"level":3,"title":"硬件准备：","slug":"硬件准备","link":"#硬件准备","children":[]},{"level":3,"title":"软件准备：","slug":"软件准备","link":"#软件准备","children":[]}]},{"level":2,"title":"本地搭建步骤","slug":"本地搭建步骤","link":"#本地搭建步骤","children":[{"level":3,"title":"1、复制项目","slug":"_1、复制项目","link":"#_1、复制项目","children":[]},{"level":3,"title":"2、创建环境","slug":"_2、创建环境","link":"#_2、创建环境","children":[]},{"level":3,"title":"3、安装依赖","slug":"_3、安装依赖","link":"#_3、安装依赖","children":[]},{"level":3,"title":"4、下载模型","slug":"_4、下载模型","link":"#_4、下载模型","children":[]},{"level":3,"title":"5、修改和初始化配置","slug":"_5、修改和初始化配置","link":"#_5、修改和初始化配置","children":[]},{"level":3,"title":"6、启动调试","slug":"_6、启动调试","link":"#_6、启动调试","children":[]},{"level":3,"title":"7、知识库测试","slug":"_7、知识库测试","link":"#_7、知识库测试","children":[]}]}],"git":{"createdTime":1708613385000,"updatedTime":1727102987000,"contributors":[{"name":"maochunguang","email":"mcg915881127@163.com","commits":2}]},"readingTime":{"minutes":3.75,"words":1124},"filePathRelative":"aigc/run-knowledge-base.md","localizedDate":"2024年2月22日","excerpt":"<p>🤖️ 一种利用 langchain 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案。</p>\\n","autoDesc":true}');export{D as comp,z as data};
